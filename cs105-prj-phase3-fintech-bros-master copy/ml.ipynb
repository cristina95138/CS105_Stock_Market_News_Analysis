{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning to predict stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it up again\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from afinn import Afinn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about the different data models used for machine learning, hypos of sentiment analysis use in prediction. Compare top1 and top25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reddit_news_df = pd.read_csv(\"Combined_News_DJIA.csv\")\n",
    "# reddit_news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in phase 2 we found out only 3 rows contain null values, so I'm dropping those again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_news_df = reddit_news_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are sentiment analysis values of the more popular headlines/articles in the r/worldnews subreddit, better stock market predictors than less popular articles?\n",
    "In the following cells I'll train logistic regression models using sentiment analysis of different Top# articles to predict whether the Dow Jones Industrial Average(DJIA) goes up or down.\n",
    "\n",
    "Initializing sentiment analysis values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "af = Afinn()\n",
    "\n",
    "top1_headlines = reddit_news_df['Top1']\n",
    "top1_headlines = [headline.replace('b\"', '').replace('b\\'', \"'\") for headline in top1_headlines]\n",
    "top1_sentiment_scores = [af.score(headline) for headline in top1_headlines]\n",
    "\n",
    "reddit_news_df['Top1 Sentiment'] = top1_sentiment_scores\n",
    "\n",
    "top5_headlines = reddit_news_df['Top5']\n",
    "top5_headlines = [headline.replace('b\"', '').replace('b\\'', \"'\") for headline in top5_headlines]\n",
    "top5_sentiment_scores = [af.score(headline) for headline in top5_headlines]\n",
    "\n",
    "reddit_news_df['Top5 Sentiment'] = top5_sentiment_scores\n",
    "\n",
    "top15_headlines = reddit_news_df['Top15']\n",
    "top15_headlines = [headline.replace('b\"', '').replace('b\\'', \"'\") for headline in top15_headlines]\n",
    "top15_sentiment_scores = [af.score(headline) for headline in top15_headlines]\n",
    "\n",
    "reddit_news_df['Top15 Sentiment'] = top15_sentiment_scores\n",
    "\n",
    "top25_headlines = reddit_news_df['Top25']\n",
    "top25_headlines = [headline.replace('b\"', '').replace('b\\'', \"'\") for headline in top25_headlines]\n",
    "top25_sentiment_scores = [af.score(headline) for headline in top25_headlines]\n",
    "\n",
    "reddit_news_df['Top25 Sentiment'] = top25_sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_train = reddit_news_df.loc[:944].copy()\n",
    "reddit_test = reddit_news_df.loc[944:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models using Top1, Top5, Top15, Top25 headlines sentiment analysis values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Top1 test set: 0.53\n",
      "Top1 model true negatives: 0\n",
      "Top1 model true positives: 322\n",
      "Top1 model false negatives: 0\n",
      "Top1 model false positives: 367 \n",
      "\n",
      "Accuracy of Top5 test set: 0.53\n",
      "Top5 model true negatives: 0\n",
      "Top5 model true positives: 322\n",
      "Top5 model false negatives: 0\n",
      "Top5 model false positives: 367 \n",
      "\n",
      "Accuracy of Top15 test set: 0.52\n",
      "Top15 model true negatives: 8\n",
      "Top15 model true positives: 314\n",
      "Top15 model false negatives: 14\n",
      "Top15 model false positives: 353 \n",
      "\n",
      "Accuracy of Top25 test set: 0.53\n",
      "Top25 model true negatives: 0\n",
      "Top25 model true positives: 322\n",
      "Top25 model false negatives: 0\n",
      "Top25 model false positives: 367 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = reddit_train[['Top1 Sentiment']]\n",
    "x_test = reddit_test[['Top1 Sentiment']]\n",
    "y_train = reddit_train[['Label']]\n",
    "y_test = reddit_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of Top1 test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('Top1 model true negatives:', tn)\n",
    "print('Top1 model true positives:', fp)\n",
    "print('Top1 model false negatives:', fn)\n",
    "print('Top1 model false positives:', tp, '\\n')\n",
    "\n",
    "################################# TOP 5 ####################################\n",
    "\n",
    "x_train = reddit_train[['Top5 Sentiment']]\n",
    "x_test = reddit_test[['Top5 Sentiment']]\n",
    "y_train = reddit_train[['Label']]\n",
    "y_test = reddit_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of Top5 test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('Top5 model true negatives:', tn)\n",
    "print('Top5 model true positives:', fp)\n",
    "print('Top5 model false negatives:', fn)\n",
    "print('Top5 model false positives:', tp, '\\n')\n",
    "\n",
    "\n",
    "################################# TOP 15 ####################################\n",
    "\n",
    "x_train = reddit_train[['Top15 Sentiment']]\n",
    "x_test = reddit_test[['Top15 Sentiment']]\n",
    "y_train = reddit_train[['Label']]\n",
    "y_test = reddit_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of Top15 test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('Top15 model true negatives:', tn)\n",
    "print('Top15 model true positives:', fp)\n",
    "print('Top15 model false negatives:', fn)\n",
    "print('Top15 model false positives:', tp, '\\n')\n",
    "\n",
    "################################# TOP 25 ####################################\n",
    "\n",
    "x_train = reddit_train[['Top25 Sentiment']]\n",
    "x_test = reddit_test[['Top25 Sentiment']]\n",
    "y_train = reddit_train[['Label']]\n",
    "y_test = reddit_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of Top25 test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('Top25 model true negatives:', tn)\n",
    "print('Top25 model true positives:', fp)\n",
    "print('Top25 model false negatives:', fn)\n",
    "print('Top25 model false positives:', tp, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "So it appears that models that use the sentiment analysis values of articles of different popularity have roughly the same accuracy of predicting the DJIA. One other observation is that sentiment analysis doesn't appear to be a good feature at first glance. All models had an accuracy of 52-53%. Though maybe this is actually good for a single feature. As we know, predicting the stock market is hard and other features need to be considered before disregarding sentiment analysis. \n",
    "\n",
    "One thing to note is the large amount of true positives the models predict. So it seems the models tend to be better at predicting when the DJIA is going to up or stay the same than predicting when its going to go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are there better features than sentiment analysis to predict the DJIA?\n",
    "alt title : Is there a better feature than sentiment analysis to predict the DJIA?\n",
    "\n",
    "In this section I will use moving average of the DJIA as a feature to train a new model.\n",
    "\n",
    "Moving average is the mean value of the DJIA over certain time periods. For example a 100-day moving average would be the mean value of the DJIA close value over the past 100 days. It's widely used by investors to aid in choosing to sell or hold a stock market position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reset df \n",
    "reddit_news_df = pd.read_csv(\"Combined_News_DJIA.csv\")\n",
    "reddit_news_df = reddit_news_df.dropna()\n",
    "\n",
    "# CSV with the DJIA data\n",
    "djia_df = pd.read_csv(\"upload_DJIA_table.csv\")\n",
    "\n",
    "# Get rows with that having matching dates.\n",
    "merged_df = pd.merge(djia_df, reddit_news_df, on=['Date'], how='inner')\n",
    "merged_df = merged_df.dropna()\n",
    "# merged_df.head()\n",
    "\n",
    "# ma_5 = merged_df['Close'].rolling(window=5).mean()\n",
    "# # print(ma_5)\n",
    "\n",
    "# ma_df = pd.DataFrame(ma_5, columns=['ma-5'])\n",
    "# ma_df = ma_df.dropna()\n",
    "\n",
    "# # print(ma_df.iloc[2])\n",
    "# ma_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models with 5-day moving average, 10-day moving average, 25-day moving average, 50-day moving average and 100-day moving average  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>...</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "      <th>ma-5</th>\n",
       "      <th>ma-10</th>\n",
       "      <th>ma-25</th>\n",
       "      <th>ma-50</th>\n",
       "      <th>ma-100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2015-09-18</td>\n",
       "      <td>16674.740234</td>\n",
       "      <td>16674.740234</td>\n",
       "      <td>16343.759766</td>\n",
       "      <td>16384.580078</td>\n",
       "      <td>341690000</td>\n",
       "      <td>16384.580078</td>\n",
       "      <td>0</td>\n",
       "      <td>Brazil's Supreme Court has banned corporate co...</td>\n",
       "      <td>Investigation finds Exxon knew about CO2's eff...</td>\n",
       "      <td>...</td>\n",
       "      <td>Farmers in northern France have been ordered t...</td>\n",
       "      <td>'Super-gonorrhoea' outbreak in Leeds</td>\n",
       "      <td>In egalitarian Sweden, richer regions reluctan...</td>\n",
       "      <td>China Is Building The Mother Of All Reputation...</td>\n",
       "      <td>Half a million children have fled attacks by t...</td>\n",
       "      <td>16341.289844</td>\n",
       "      <td>16262.884863</td>\n",
       "      <td>16732.680195</td>\n",
       "      <td>17222.747949</td>\n",
       "      <td>17041.170156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2015-09-17</td>\n",
       "      <td>16738.080078</td>\n",
       "      <td>16933.429688</td>\n",
       "      <td>16639.929688</td>\n",
       "      <td>16674.740234</td>\n",
       "      <td>129600000</td>\n",
       "      <td>16674.740234</td>\n",
       "      <td>0</td>\n",
       "      <td>Efficiency up, turnover down: Sweden experimen...</td>\n",
       "      <td>7.9-Magnitude Earthquake Strikes off the Coast...</td>\n",
       "      <td>...</td>\n",
       "      <td>Threatened, starved: Cook reveals life at Saud...</td>\n",
       "      <td>Global study reveals soaring antibiotic resist...</td>\n",
       "      <td>Burkina Faso 'coup': Presidential guard dissol...</td>\n",
       "      <td>Malicious Cisco router backdoor found on 79 mo...</td>\n",
       "      <td>Russian Authorities Close Down American Center...</td>\n",
       "      <td>16435.973828</td>\n",
       "      <td>16303.157910</td>\n",
       "      <td>16700.103398</td>\n",
       "      <td>17199.974941</td>\n",
       "      <td>17048.770156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>16599.509766</td>\n",
       "      <td>16755.980469</td>\n",
       "      <td>16593.900391</td>\n",
       "      <td>16739.949219</td>\n",
       "      <td>99620000</td>\n",
       "      <td>16739.949219</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuna and mackerel populations suffer catastrop...</td>\n",
       "      <td>Australian Government introduces \"No Jab No Pa...</td>\n",
       "      <td>...</td>\n",
       "      <td>Flying Korea's farmed dogs to safety - \"Our go...</td>\n",
       "      <td>Turkish presidents office says insulting presi...</td>\n",
       "      <td>Saudi suspends Binladen group over Mecca crane...</td>\n",
       "      <td>Anheuser-Busch InBev, the maker of Budweiser a...</td>\n",
       "      <td>China stocks resume sharp slide as economic wo...</td>\n",
       "      <td>16527.985742</td>\n",
       "      <td>16348.682812</td>\n",
       "      <td>16682.956992</td>\n",
       "      <td>17178.506113</td>\n",
       "      <td>17056.025850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2015-09-15</td>\n",
       "      <td>16382.580078</td>\n",
       "      <td>16644.109375</td>\n",
       "      <td>16382.580078</td>\n",
       "      <td>16599.849609</td>\n",
       "      <td>93050000</td>\n",
       "      <td>16599.849609</td>\n",
       "      <td>1</td>\n",
       "      <td>Egyptian Billionaire who wants to purchase pri...</td>\n",
       "      <td>The UN Says US Drone Strikes in Yemen Targetin...</td>\n",
       "      <td>...</td>\n",
       "      <td>Canadian banks helping clients bend rules to m...</td>\n",
       "      <td>Poland &amp;amp; Sweden agree to intensify militar...</td>\n",
       "      <td>North Korea 'restarts nuclear operations' | BBC</td>\n",
       "      <td>Teen Arrested for Planning Alleged ISIS-Inspir...</td>\n",
       "      <td>'Syria Is Emptying'</td>\n",
       "      <td>16581.861719</td>\n",
       "      <td>16403.754785</td>\n",
       "      <td>16658.266601</td>\n",
       "      <td>17154.259316</td>\n",
       "      <td>17061.753848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2015-09-14</td>\n",
       "      <td>16450.859375</td>\n",
       "      <td>16450.859375</td>\n",
       "      <td>16330.870117</td>\n",
       "      <td>16370.959961</td>\n",
       "      <td>92660000</td>\n",
       "      <td>16370.959961</td>\n",
       "      <td>0</td>\n",
       "      <td>Malcom Turnbull becomes Prime Minister of Aust...</td>\n",
       "      <td>El Nino set to be strongest ever. The most pow...</td>\n",
       "      <td>...</td>\n",
       "      <td>Taliban storms Afghan jail with suicide bomber...</td>\n",
       "      <td>NASA Launching 4K TV Channel</td>\n",
       "      <td>The Egyptian army announced it has killed 64 a...</td>\n",
       "      <td>Oxfam: Increasing inequality plunging millions...</td>\n",
       "      <td>Czech PM insists migrant quotas 'won't work'</td>\n",
       "      <td>16554.015820</td>\n",
       "      <td>16440.661816</td>\n",
       "      <td>16623.883437</td>\n",
       "      <td>17125.824922</td>\n",
       "      <td>17063.413750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date          Open          High           Low         Close  \\\n",
       "198  2015-09-18  16674.740234  16674.740234  16343.759766  16384.580078   \n",
       "199  2015-09-17  16738.080078  16933.429688  16639.929688  16674.740234   \n",
       "200  2015-09-16  16599.509766  16755.980469  16593.900391  16739.949219   \n",
       "201  2015-09-15  16382.580078  16644.109375  16382.580078  16599.849609   \n",
       "202  2015-09-14  16450.859375  16450.859375  16330.870117  16370.959961   \n",
       "\n",
       "        Volume     Adj Close  Label  \\\n",
       "198  341690000  16384.580078      0   \n",
       "199  129600000  16674.740234      0   \n",
       "200   99620000  16739.949219      1   \n",
       "201   93050000  16599.849609      1   \n",
       "202   92660000  16370.959961      0   \n",
       "\n",
       "                                                  Top1  \\\n",
       "198  Brazil's Supreme Court has banned corporate co...   \n",
       "199  Efficiency up, turnover down: Sweden experimen...   \n",
       "200  Tuna and mackerel populations suffer catastrop...   \n",
       "201  Egyptian Billionaire who wants to purchase pri...   \n",
       "202  Malcom Turnbull becomes Prime Minister of Aust...   \n",
       "\n",
       "                                                  Top2      ...       \\\n",
       "198  Investigation finds Exxon knew about CO2's eff...      ...        \n",
       "199  7.9-Magnitude Earthquake Strikes off the Coast...      ...        \n",
       "200  Australian Government introduces \"No Jab No Pa...      ...        \n",
       "201  The UN Says US Drone Strikes in Yemen Targetin...      ...        \n",
       "202  El Nino set to be strongest ever. The most pow...      ...        \n",
       "\n",
       "                                                 Top21  \\\n",
       "198  Farmers in northern France have been ordered t...   \n",
       "199  Threatened, starved: Cook reveals life at Saud...   \n",
       "200  Flying Korea's farmed dogs to safety - \"Our go...   \n",
       "201  Canadian banks helping clients bend rules to m...   \n",
       "202  Taliban storms Afghan jail with suicide bomber...   \n",
       "\n",
       "                                                 Top22  \\\n",
       "198               'Super-gonorrhoea' outbreak in Leeds   \n",
       "199  Global study reveals soaring antibiotic resist...   \n",
       "200  Turkish presidents office says insulting presi...   \n",
       "201  Poland &amp; Sweden agree to intensify militar...   \n",
       "202                       NASA Launching 4K TV Channel   \n",
       "\n",
       "                                                 Top23  \\\n",
       "198  In egalitarian Sweden, richer regions reluctan...   \n",
       "199  Burkina Faso 'coup': Presidential guard dissol...   \n",
       "200  Saudi suspends Binladen group over Mecca crane...   \n",
       "201    North Korea 'restarts nuclear operations' | BBC   \n",
       "202  The Egyptian army announced it has killed 64 a...   \n",
       "\n",
       "                                                 Top24  \\\n",
       "198  China Is Building The Mother Of All Reputation...   \n",
       "199  Malicious Cisco router backdoor found on 79 mo...   \n",
       "200  Anheuser-Busch InBev, the maker of Budweiser a...   \n",
       "201  Teen Arrested for Planning Alleged ISIS-Inspir...   \n",
       "202  Oxfam: Increasing inequality plunging millions...   \n",
       "\n",
       "                                                 Top25          ma-5  \\\n",
       "198  Half a million children have fled attacks by t...  16341.289844   \n",
       "199  Russian Authorities Close Down American Center...  16435.973828   \n",
       "200  China stocks resume sharp slide as economic wo...  16527.985742   \n",
       "201                                'Syria Is Emptying'  16581.861719   \n",
       "202       Czech PM insists migrant quotas 'won't work'  16554.015820   \n",
       "\n",
       "            ma-10         ma-25         ma-50        ma-100  \n",
       "198  16262.884863  16732.680195  17222.747949  17041.170156  \n",
       "199  16303.157910  16700.103398  17199.974941  17048.770156  \n",
       "200  16348.682812  16682.956992  17178.506113  17056.025850  \n",
       "201  16403.754785  16658.266601  17154.259316  17061.753848  \n",
       "202  16440.661816  16623.883437  17125.824922  17063.413750  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find moving averages\n",
    "merged_df[\"ma-5\"] = merged_df['Close'].rolling(window=5).mean()\n",
    "merged_df[\"ma-10\"] = merged_df['Close'].rolling(window=10).mean()\n",
    "merged_df[\"ma-25\"] = merged_df['Close'].rolling(window=25).mean()\n",
    "merged_df[\"ma-50\"] = merged_df['Close'].rolling(window=50).mean()\n",
    "merged_df[\"ma-100\"] = merged_df['Close'].rolling(window=100).mean()\n",
    "\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "ma_train = merged_df.loc[:1300].copy()\n",
    "ma_test = merged_df.loc[1300:].copy()\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 5-day moving average test set: 0.55\n",
      "5-day moving average model true negatives: 0\n",
      "5-day moving average model true positives: 311\n",
      "5-day moving average model false negatives: 0\n",
      "5-day moving average model false positives: 375 \n",
      "\n",
      "Accuracy of 10-day moving average test set: 0.55\n",
      "10-day moving average model true negatives: 0\n",
      "10-day moving average model true positives: 311\n",
      "10-day moving average model false negatives: 0\n",
      "10-day moving average model false positives: 375 \n",
      "\n",
      "Accuracy of 25-day moving average test set: 0.55\n",
      "25-day moving average model true negatives: 0\n",
      "25-day moving average model true positives: 311\n",
      "25-day moving average model false negatives: 0\n",
      "25-day moving average model false positives: 375 \n",
      "\n",
      "Accuracy of 50-day moving average test set: 0.55\n",
      "50-day moving average model true negatives: 0\n",
      "50-day moving average model true positives: 311\n",
      "50-day moving average model false negatives: 0\n",
      "50-day moving average model false positives: 375 \n",
      "\n",
      "Accuracy of 100-day moving average test set: 0.55\n",
      "100-day moving average model true negatives: 0\n",
      "100-day moving average model true positives: 311\n",
      "100-day moving average model false negatives: 0\n",
      "100-day moving average model false positives: 375 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = ma_train[['ma-5']]\n",
    "x_test = ma_test[['ma-5']]\n",
    "y_train = ma_train[['Label']]\n",
    "y_test = ma_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of 5-day moving average test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('5-day moving average model true negatives:', tn)\n",
    "print('5-day moving average model true positives:', fp)\n",
    "print('5-day moving average model false negatives:', fn)\n",
    "print('5-day moving average model false positives:', tp, '\\n')\n",
    "\n",
    "############################ 10-day model ####################################\n",
    "\n",
    "x_train = ma_train[['ma-10']]\n",
    "x_test = ma_test[['ma-10']]\n",
    "y_train = ma_train[['Label']]\n",
    "y_test = ma_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of 10-day moving average test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('10-day moving average model true negatives:', tn)\n",
    "print('10-day moving average model true positives:', fp)\n",
    "print('10-day moving average model false negatives:', fn)\n",
    "print('10-day moving average model false positives:', tp, '\\n')\n",
    "\n",
    "############################ 25-day model ####################################\n",
    "\n",
    "x_train = ma_train[['ma-25']]\n",
    "x_test = ma_test[['ma-25']]\n",
    "y_train = ma_train[['Label']]\n",
    "y_test = ma_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of 25-day moving average test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('25-day moving average model true negatives:', tn)\n",
    "print('25-day moving average model true positives:', fp)\n",
    "print('25-day moving average model false negatives:', fn)\n",
    "print('25-day moving average model false positives:', tp, '\\n')\n",
    "\n",
    "############################ 50-day model ####################################\n",
    "\n",
    "x_train = ma_train[['ma-50']]\n",
    "x_test = ma_test[['ma-50']]\n",
    "y_train = ma_train[['Label']]\n",
    "y_test = ma_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of 50-day moving average test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('50-day moving average model true negatives:', tn)\n",
    "print('50-day moving average model true positives:', fp)\n",
    "print('50-day moving average model false negatives:', fn)\n",
    "print('50-day moving average model false positives:', tp, '\\n')\n",
    "\n",
    "############################ 100-day model ####################################\n",
    "\n",
    "x_train = ma_train[['ma-100']]\n",
    "x_test = ma_test[['ma-100']]\n",
    "y_train = ma_train[['Label']]\n",
    "y_test = ma_test[['Label']]\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg.fit(x_train, y_train.values.ravel())\n",
    "y_pred = logReg.predict(x_test)\n",
    "print('Accuracy of 100-day moving average test set: {:.2f}'.format(logReg.score(x_test, y_test)))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print('100-day moving average model true negatives:', tn)\n",
    "print('100-day moving average model true positives:', fp)\n",
    "print('100-day moving average model false negatives:', fn)\n",
    "print('100-day moving average model false positives:', tp, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "So it appears that models that use moving averages are only slighly better at predicting the DJIA, by 1% greater accuracy. Though just like the sentiment analysis based models, these models are really good at finding true positives and false positives. Leading me to believe that the models have a hard time finding a connection between the features and the DJIA value going down. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
